{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olO3RsEsAxxD"
   },
   "source": [
    "# Introduction to Natural Language Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQqmBN6nMQ8o"
   },
   "source": [
    "## Wikipedia\n",
    "\n",
    "wikipedia package: https://towardsdatascience.com/wikipedia-api-for-python-241cfae09f1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNNpdXcuJFVJ",
    "outputId": "bc364cb7-cef6-41db-9187-eca8ac1d1f1c"
   },
   "outputs": [],
   "source": [
    "#!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0xGEkMEAoMM"
   },
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6Ggf-9YJGux",
    "outputId": "6d80c453-6587-4477-f7b9-5b28d8916730"
   },
   "outputs": [],
   "source": [
    "# resultados\n",
    "wikipedia.search('Fake News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRsroWbAJgWW",
    "outputId": "785929a5-023d-4d40-c359-f579addc3aa1"
   },
   "outputs": [],
   "source": [
    "# suggestion\n",
    "print(wikipedia.suggest('Fake ne'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "PIY8O6mGJyv2",
    "outputId": "316c1dc9-45bb-4749-a385-43a1d95fcc6b"
   },
   "outputs": [],
   "source": [
    "# summary\n",
    "wikipedia.summary('Fake News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "id": "ZDAioCY1KLjA",
    "outputId": "c6bfcbf9-0459-4b3c-d04c-5e5594de1933",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DesambiguationError\n",
    "#wikipedia.summary('News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "jsLvMOXMKY5I",
    "outputId": "b0b1651b-d770-4b14-9fd7-a12d215dfa7b"
   },
   "outputs": [],
   "source": [
    "# set language\n",
    "wikipedia.set_lang('pt')\n",
    "wikipedia.summary('Fake News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDTCjUJwKpEz",
    "outputId": "0f2663ee-7c95-4eb6-ab93-ad7b92142e66"
   },
   "outputs": [],
   "source": [
    "# language support\n",
    "#wikipedia.languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DujAe7F0KvsY",
    "outputId": "c52134ee-fcff-4e45-e14f-bdb8272286e6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# access page\n",
    "fake = wikipedia.page('Fake News')\n",
    "print('title')\n",
    "print(fake.title)\n",
    "\n",
    "print('url')\n",
    "print(fake.url)\n",
    "\n",
    "print('content')\n",
    "print(fake.content)\n",
    "\n",
    "print('images')\n",
    "print(fake.images)\n",
    "\n",
    "print('links')\n",
    "print(fake.links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mFwCGOkMZUP"
   },
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqRnJhV6NEJS"
   },
   "outputs": [],
   "source": [
    "# package regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiPltoGeL-9F"
   },
   "outputs": [],
   "source": [
    "content = fake.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNAMBIjQMd1J",
    "outputId": "1fb42002-9cb3-4f91-ab95-363bc8764185"
   },
   "outputs": [],
   "source": [
    "# split words by space\n",
    "spaces = r'\\s+'\n",
    "split_spaces = re.split(spaces, content)\n",
    "len(split_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcOUYRGHNV85"
   },
   "outputs": [],
   "source": [
    "# numbers\n",
    "digits = r'\\d+'\n",
    "finded_digits = re.findall(digits, content)\n",
    "finded_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_-hZCBCOK1s"
   },
   "outputs": [],
   "source": [
    "# dots and commas\n",
    "dots_coma = r'[.,]'\n",
    "split_dotcoma = re.split(dots_coma, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MW2KkKT7O4mC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comma_space = r'[,]\\s+'\n",
    "split_comma_space = re.split(comma_space, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8JrwfxQPzjN",
    "outputId": "a60ef4f3-c0d8-4b3e-f2ec-4e97f076ae91"
   },
   "outputs": [],
   "source": [
    "# find all 'fake news'\n",
    "re.findall('fake [A-Za-z]*', content)  # use all letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rW-9G--HqzuJ",
    "outputId": "05876ed8-1c1c-4398-e069-4581bd0c8ecb"
   },
   "outputs": [],
   "source": [
    "# re.match: return the first match of a substring found, but re.match searches only from the beginning of the string\n",
    "# re.search: _same_, but searches for the whole string\n",
    "print(re.match('news', content))\n",
    "print(re.search('news', content))\n",
    "print(re.search('politico', content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yno7n3Lbr9zB",
    "outputId": "ab3c5fbf-0dc9-4223-d15e-f3341e9d33ac"
   },
   "outputs": [],
   "source": [
    "# start and end method\n",
    "news = re.search('news', content)\n",
    "print(news.start(), news.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_U-nx7ZdsMwG",
    "outputId": "00101a44-25c4-4fe6-84dc-bf71d14892f1"
   },
   "outputs": [],
   "source": [
    "# search for anything in parenteses\n",
    "pattern = r'\\(.*?\\)'\n",
    "print('re.search:', re.search(pattern, content))\n",
    "print('re.findall:', re.findall(pattern, content)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "PAtRhO6FvjYF",
    "outputId": "1303b98d-7ed3-41c3-8ee9-6898daf9ddce"
   },
   "outputs": [],
   "source": [
    "content[16:77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11DjE2sbwMU3",
    "outputId": "d87697a2-3d42-4111-dda9-1d80d76321e5"
   },
   "outputs": [],
   "source": [
    "# search for anything in quotations marks\n",
    "pattern_qm = r'\"(.*?)\"'\n",
    "print('re.search:', re.search(pattern_qm, content))\n",
    "print('re.findall:', re.findall(pattern_qm, content)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMNYu4ozyonn",
    "outputId": "d6deabb4-e386-4a1d-dc9b-6d500b6611bd"
   },
   "outputs": [],
   "source": [
    "# seach for sections between \"==\"\n",
    "pattern_qm = r'\\==(\\s.*\\s)\\=='                    # () define groups of patterns, [] define explicity characters\n",
    "print('re.search:', re.search(pattern_qm, content))\n",
    "print('re.findall sections:', re.findall(pattern_qm, content)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45RL4H1HXs-A"
   },
   "source": [
    "## Tokenization\n",
    " Transformar strings e documentos into tokens (smaller chunks)\n",
    "\n",
    " Library: NLTK - Natural Language Toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0b7FSjqXvbT"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSF5kugfobpQ",
    "outputId": "3d94c2cd-a446-4349-964a-3bc90f499dfd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA5DUNrtoLOD"
   },
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "sentences = nltk.tokenize.sent_tokenize(content, 'portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6nBX7s5pWS8",
    "outputId": "5d4dd3bc-8125-4d21-e2ec-c45b6e490a9c"
   },
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuW3HWfNo9gz"
   },
   "outputs": [],
   "source": [
    "# tokenize wordds\n",
    "first_tokens = nltk.word_tokenize(sentences[0], 'portuguese')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcDSDSsspIN8",
    "outputId": "f4fb9c82-d864-4911-c9b2-c6bda13017cb"
   },
   "outputs": [],
   "source": [
    "len(first_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "811lV2GOp35T",
    "outputId": "6285d3c8-1b28-4657-cb86-a9d1f3abc983"
   },
   "outputs": [],
   "source": [
    "# tokenize all text - unique\n",
    "len(set(nltk.word_tokenize(content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYzPpdlp5SG6"
   },
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kz1er7Ea7lZf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXF7DuBXpkL1",
    "outputId": "275250db-e8bf-40e2-a926-75ecaddacd4a"
   },
   "outputs": [],
   "source": [
    "len(nltk.regexp_tokenize(content, '\\w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoXISt417D8s"
   },
   "outputs": [],
   "source": [
    "len_words = [len(w) for w in set(nltk.regexp_tokenize(content, '\\w+'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "e0tL9DZf7i0a",
    "outputId": "5f5239ae-12c1-4779-e410-6a00c5d7262b"
   },
   "outputs": [],
   "source": [
    "plt.hist(len_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "- basic method for finding topics in a text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verifying variables in environment\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# tokens\n",
    "tokens = nltk.word_tokenize(content)\n",
    "\n",
    "# creating a counter\n",
    "count_tokens = Counter(tokens)\n",
    "\n",
    "# all elements of counter\n",
    "count_tokens.elements\n",
    "\n",
    "# remove zero and negative counts\n",
    "+count_tokens\n",
    "\n",
    "# most commons\n",
    "count_tokens.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple preprocessing\n",
    "\n",
    "- tokenization\n",
    "- lowercasing \n",
    "- lemmatization/stemming\n",
    "- removing stopwords, punctuation, or unwanted tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module for stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# only portuguse stopwords\n",
    "pt_stopwords = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization and lowercasing at same time\n",
    "tokens_lower = [w for w in nltk.word_tokenize(content.lower()) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "no_stops = [t for t in tokens_lower if t not in pt_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter\n",
    "no_stops_counter = Counter(no_stops)\n",
    "no_stops_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming instance\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# act\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# new counter with words lemmatized\n",
    "stem_counter = Counter(lemmatized)\n",
    "stem_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update stopwords without 'ser', 'sobre'\n",
    "pt_stopwords.update(['ser', 'sobre'])\n",
    "\n",
    "# again\n",
    "no_stops = [t for t in tokens_lower if t not in pt_stopwords]\n",
    "\n",
    "# again update\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# new counter with words lemmatized - update no_stops\n",
    "stem_counter = Counter(lemmatized)\n",
    "stem_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim - word vector or word embedding\n",
    "\n",
    "Transform words in numbers (0 and 1), to calculate distance between words, represented in a multi-dimensional array. We can see the relations between words. We use a dictionary to create a corpus of tokens id and frequency of each id.\n",
    "\n",
    "_\"Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.\"_\n",
    "\n",
    "In order to work on text documents, Gensim requires the words (aka tokens) be converted to unique ids. In order to achieve that, Gensim lets you create a Dictionary object that maps each word to a unique id. Corpus is a 'collection of documents as a bag of words’\n",
    "\n",
    "reference site: https://www.machinelearningplus.com/nlp/gensim-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary considering each sentence a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gensim dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# creating differences 'documents' - it's sentences of fake news content\n",
    "docs = [nltk.word_tokenize(c.lower()) for c in nltk.sent_tokenize(content)]\n",
    "\n",
    "# removing stopwords and punctuation (isalpha())\n",
    "doc_no_stop = []\n",
    "docs_no_stop = []\n",
    "\n",
    "for doc in docs:\n",
    "    doc_no_stop = [d for d in doc if d not in pt_stopwords if d.isalpha()]\n",
    "    docs_no_stop.append(doc_no_stop)\n",
    "    \n",
    "# stemming words in docs\n",
    "doc_stem = []\n",
    "docs_stem = []\n",
    "\n",
    "for doc in docs_no_stop:\n",
    "    doc_stem = [wordnet_lemmatizer.lemmatize(d) for d in doc]\n",
    "    docs_stem.append(doc_stem)\n",
    "\n",
    "# creating id for each token\n",
    "dictionary = Dictionary(docs_stem)\n",
    "\n",
    "# print five tokens - between 100 and 105\n",
    "print(list(dictionary.token2id.items())[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can update an existing dictionary with other docs with `dictionary.add_documents(text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a gensim corpus of docs\n",
    "import random\n",
    "\n",
    "corpus_sent_docs = [dictionary.doc2bow(doc) for doc in docs_stem]\n",
    "\n",
    "# print a random sample of one document in corpus\n",
    "random.sample(corpus_sent_docs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 words\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# if not have the key, value is 0\n",
    "total_word_count = defaultdict(int)\n",
    "\n",
    "# sum of words\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus_sent_docs):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary considering whole text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_no_stops = ' '.join(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_preprocess is necessary to create dictionary with single file/text\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# 'gambiarra' in list([content])\n",
    "dict_single_text = Dictionary(simple_preprocess(c) for c in list([content_no_stops]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(random.sample(list(dict_single_text.token2id.items()), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get specific id\n",
    "dict_single_text.token2id.get('notícias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for single text corpus\n",
    "\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, str_, dictionary):\n",
    "        self.str = str_\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for line in self.str:\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus_singlet = BoWCorpus(list([content_no_stops]), dictionary=dict_single_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print five first \n",
    "bow_corpus = [line for line in bow_corpus_singlet][0]\n",
    "print(bow_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorted by scond item in list of tuples, to get most frequent words\n",
    "sort_corpus = sorted(bow_corpus, key= lambda w: w[1], reverse=True)\n",
    "\n",
    "# print the top 5 words\n",
    "for word_id, word_count in sort_corpus[:5]:\n",
    "    print('word:', dict_single_text[word_id]+',', 'count:',word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rha5X71t4yLY"
   },
   "source": [
    "# Twitter Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-FI85oM40W6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Pq-RqUd45dm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViXZYxjL49Jl"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PNL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
